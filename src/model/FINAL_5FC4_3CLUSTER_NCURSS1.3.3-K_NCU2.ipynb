{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU_number<br/>\n",
    "batch_size = 64<br/>\n",
    "CH_N = 3<br/>\n",
    "def write_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Initial\n",
    "   https://arxiv.org/pdf/1803.09820.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.7.0\n",
      "Eager mode:  True\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import random as rd\n",
    "import cv2\n",
    "import glob, shutil, openpyxl, datetime\n",
    "\n",
    "from tensorflow import float32\n",
    "import math\n",
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "device = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(\n",
    "    device, True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this code below is used to create folders and xlx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirp = 'cabbage_2022-05-09'\n",
      "dst = 'cabbage.xlsx'\n",
      "sheet_name = 'cabbage_3c_12'\n"
     ]
    }
   ],
   "source": [
    "root = ''\n",
    "date = str(datetime.date.today())\n",
    "foldername = 'cabbage_' + date\n",
    "dirp = root + foldername\n",
    "if not os.path.exists(dirp):\n",
    "    os.mkdir(dirp)\n",
    "    ulogdir = dirp + '/logs'\n",
    "    os.mkdir(dirp + '/Models')\n",
    "    os.mkdir(dirp + '/Val')\n",
    "    os.mkdir(dirp + '/Test')\n",
    "\n",
    "# ## TODO ask Ida what is the template is ?\n",
    "source = r'template.xlsx'\n",
    "dst = root + r'cabbage.xlsx'\n",
    "sheet_name = 'cabbage_3c_'\n",
    "\n",
    "if not os.path.exists(dst):\n",
    "    shutil.copyfile(source, dst)\n",
    "workbook = openpyxl.load_workbook(dst)\n",
    "sheet = workbook['sheet1']\n",
    "workbook.copy_worksheet(sheet)\n",
    "sh_name = workbook.sheetnames  # 获取所有sheet\n",
    "sh = workbook[sh_name[-1]]\n",
    "sh.title = sheet_name\n",
    "sh['A1'] = sheet_name\n",
    "sh['C1'] = foldername\n",
    "sh_name = workbook.sheetnames\n",
    "sheet_name = sh_name[-1]\n",
    "workbook.save(dst)\n",
    "\n",
    "print('dirp = \\'' + dirp + '\\'')\n",
    "print('dst = \\'' + dst + '\\'')\n",
    "print('sheet_name = \\'' + sheet_name + '\\'')\n",
    "# print(\"tensorboard --logdir=\\\"D:/py_project/twcc/{}\\\" --bind_all\".format(foldername))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "assert len(gpu) == 1\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "# TODO this should be made as CONSTANT\n",
    "BATCH_SIZE = 16 #64\n",
    "CH_N = 3\n",
    "imgs = 256\n",
    "print(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n",
      "10800 10800\n"
     ]
    }
   ],
   "source": [
    "# CHANGE THE LOCATION OF TRAINING & VALIDATION IMAGES\n",
    "imgfiles = [f for f in glob.glob(r'/home/kuro/Documents/cabbage/FOLD1_3CLUSTER/F1C1_IMG_3/*.png')]\n",
    "mskfiles = [f for f in glob.glob(r'/home/kuro/Documents/cabbage/FOLD1_3CLUSTER/F1C1_MASK_3/*.png')]\n",
    "\n",
    "imgfiles.sort()\n",
    "mskfiles.sort()\n",
    "img = cv2.imread(imgfiles[5800])\n",
    "mask = cv2.imread(mskfiles[5901])\n",
    "# print(np.unique(img))\n",
    "print(np.unique(mask))\n",
    "print(len(imgfiles), len(mskfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(imgfiles[500])\n",
    "print(max(np.unique(img)))\n",
    "print(img.shape)\n",
    "msk = cv2.imread(mskfiles[500])\n",
    "print(max(np.unique(msk)))\n",
    "print(msk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# TODO what is 2160?\n",
    "n_pic = len(imgfiles) // 2160\n",
    "whole_imgl = [{} for _ in range(n_pic)]\n",
    "whole_mskl = [{} for _ in range(n_pic)]\n",
    "for r, i in enumerate(zip(imgfiles, mskfiles)):\n",
    "    whole_imgl[r % n_pic][int(i[0].split('/')[-1].split('_')[0])] = (i[0])\n",
    "    whole_mskl[r % n_pic][int(i[0].split('/')[-1].split('_')[0])] = (i[1])\n",
    "w_imgl = [[x.get(i) for i in range(1, len(x) + 1, 1)] for x in whole_imgl]\n",
    "w_mskl = [[x.get(i) for i in range(1, len(x) + 1, 1)] for x in whole_mskl]\n",
    "print(len(w_imgl))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imggen(imgpath, mskpath, stop, imgs, aug=False, whole=False):\n",
    "    i = 0\n",
    "    if whole:\n",
    "        i = 1\n",
    "    while i < stop:\n",
    "        #img = tiffile.imread(imgpath[i].decode())\n",
    "        img = tf.io.read_file(imgpath[i])\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.divide(img, 255)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        # img = tf.image.resize(img,(imgs,imgs))\n",
    "        msk = tf.io.read_file(mskpath[i])\n",
    "        msk = tf.image.decode_jpeg(msk, channels=1)\n",
    "        msk = tf.image.convert_image_dtype(msk, tf.float32)\n",
    "        # msk = tf.image.resize(msk,(imgs,imgs))\n",
    "        msk = tf.math.logical_and(msk < 255, msk > 0)\n",
    "        msk = tf.cast(msk, tf.float32)\n",
    "\n",
    "        if aug:\n",
    "            if rd.random() > 0.5:\n",
    "                img = tf.image.flip_left_right(img)\n",
    "                msk = tf.image.flip_left_right(msk)\n",
    "            if rd.random() > 0.5:\n",
    "                img = tf.image.flip_up_down(img)\n",
    "                msk = tf.image.flip_up_down(msk)\n",
    "            if rd.random() > 0.6:\n",
    "                img = tf.image.random_brightness(img, 0.3)\n",
    "\n",
    "        img = tf.image.per_image_standardization(img)\n",
    "        #if CH_N<4:\n",
    "        #img = tf.slice(img,[0,0,0],[-1,-1,CH_N])\n",
    "\n",
    "        i += 1\n",
    "        yield img, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fixx(img, msk):\n",
    "    img.set_shape([imgs, imgs, CH_N])\n",
    "    msk.set_shape([imgs, imgs, 1])\n",
    "    return img, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(bs, imgs, chn, trains, vals):\n",
    "    trilist = [x for r in range(len(w_imgl)) if r in trains for x in w_imgl[r]]\n",
    "    trmlist = [x for r in range(len(w_mskl)) if r in trains for x in w_mskl[r]]\n",
    "    valilist = [x for r in range(len(w_imgl)) if r in vals for x in w_imgl[r]]\n",
    "    valmlist = [x for r in range(len(w_mskl)) if r in vals for x in w_mskl[r]]\n",
    "\n",
    "    a = list(zip(trilist, trmlist))\n",
    "    #     rd.shuffle(a)\n",
    "    trilist, trmlist = zip(*a)\n",
    "\n",
    "    tr_ds = tf.data.Dataset.from_generator(imggen, (float32, float32),\n",
    "                                           args=[trilist, trmlist, len(trmlist), imgs, True])\n",
    "    tr = tr_ds.map(fixx, num_parallel_calls=tf.data.experimental.AUTOTUNE).repeat().batch(bs)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_generator(imggen, (float32, float32),\n",
    "                                            args=[valilist, valmlist, len(valmlist), imgs, False])\n",
    "    val = val_ds.map(fixx).batch(bs)\n",
    "\n",
    "    return tr, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testds(bs, imgs, chn, valn, indx):\n",
    "    Test_ds = tf.data.Dataset.from_generator(imggen, (float32, float32),\n",
    "                                             args=[w_imgl[valn[indx]], w_mskl[valn[indx]], len(w_imgl[valn[indx]]),\n",
    "                                                   imgs, False])\n",
    "    Testing = Test_ds.map(fixx).batch(bs)\n",
    "    return Testing\n",
    "\n",
    "\n",
    "def create_valds(bs, imgs, chn, valn, indx):\n",
    "    Test_ds = tf.data.Dataset.from_generator(imggen, (float32, float32),\n",
    "                                             args=[w_imgl[valn[indx]], w_mskl[valn[indx]], len(w_imgl[valn[indx]]),\n",
    "                                                   imgs, False])\n",
    "\n",
    "    Testing = Test_ds.map(fixx).batch(bs)\n",
    "    return Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Unet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_0(y_true, y_pred, smooth=1e-3):\n",
    "    y_true = K.cast(K.squeeze(K.one_hot(K.cast(y_true, 'int64'), 2), -2), 'float32')\n",
    "    y_true = tf.slice(y_true, [0, 0, 0, 0], [-1, -1, -1, 1])\n",
    "    y_pred = tf.slice(y_pred, [0, 0, 0, 0], [-1, -1, -1, 1])\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred, [1, 2, 3]) - intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def iou_1(y_true, y_pred, smooth=1e-3):\n",
    "    y_true = K.cast(K.squeeze(K.one_hot(K.cast(y_true, 'int64'), 2), -2), 'float32')\n",
    "    y_true = tf.slice(y_true, [0, 0, 0, 1], [-1, -1, -1, -1])\n",
    "    y_pred = tf.slice(y_pred, [0, 0, 0, 1], [-1, -1, -1, -1])\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "    union = K.sum(y_true, [1, 2, 3]) + K.sum(y_pred, [1, 2, 3]) - intersection\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(y_true, y_pred, argm=True):\n",
    "    if argm:\n",
    "        y_pred2 = tf.reshape(tf.argmax(y_pred, axis=-1), [-1])\n",
    "    else:\n",
    "        y_pred2 = tf.reshape(y_pred, [-1])\n",
    "    y_true2 = tf.reshape(y_true, [-1])\n",
    "    conf = tf.cast(tf.math.confusion_matrix(y_true2, y_pred2, num_classes=2), float32)\n",
    "    actual_ratings_hist = tf.reduce_sum(conf, axis=1)\n",
    "    pred_ratings_hist = tf.reduce_sum(conf, axis=0)\n",
    "\n",
    "    #print(conf,actual_ratings_hist,pred_ratings_hist)\n",
    "    nb_ratings = tf.shape(conf)[0]\n",
    "    weight_mtx = tf.zeros([nb_ratings, nb_ratings], dtype=tf.float32)\n",
    "    diagonal = tf.ones([nb_ratings], dtype=tf.float32)\n",
    "    weight_mtx = tf.linalg.set_diag(weight_mtx, diagonal=diagonal)\n",
    "    gc = actual_ratings_hist * pred_ratings_hist\n",
    "    conf = tf.cast(conf, float32)\n",
    "    totaln = tf.cast(tf.shape(y_true2)[0], float32)\n",
    "    up = tf.cast(totaln * tf.reduce_sum(conf * weight_mtx), float32) - tf.cast(tf.reduce_sum(gc), float32)\n",
    "    down = tf.cast(totaln ** 2 - tf.reduce_sum(gc), float32)\n",
    "    #print(weight_mtx,gc,conf,up,down)\n",
    "\n",
    "    if tf.math.is_nan(up / down):\n",
    "        return 0.\n",
    "    return up / down\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cost_function(y, y_pred):\n",
    "    WD = 10.\n",
    "    y = K.cast(K.squeeze(K.one_hot(K.cast(y, 'int64'), 2), -2), 'float32')\n",
    "\n",
    "    return K.sum(y * (-1) * tf.convert_to_tensor([1., WD]) * tf.math.log(y_pred + 1e-9), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size):\n",
    "    return [layers.Conv2DTranspose(filters, size, strides=2, padding='same'), layers.BatchNormalization(),\n",
    "            layers.ReLU()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_basem():\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=[imgs, imgs, 3], include_top=False)\n",
    "\n",
    "    vgg16_layer_names = [\n",
    "        'block1_conv2',  # 1\n",
    "        'block2_conv2',  # 1/2\n",
    "        'block3_conv3',  # 1/4\n",
    "        'block4_conv3',  # 1/8\n",
    "        'block5_conv3',  # 1/16\n",
    "    ]\n",
    "    vgg16_layer_names.reverse()\n",
    "    vgglayers = base_model.outputs + [base_model.get_layer(name).output for name in vgg16_layer_names]\n",
    "\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=vgglayers)\n",
    "    # down_stack.summary()\n",
    "    return down_stack\n",
    "\n",
    "\n",
    "def def_unet(classes, height, width, channels, base_model, name):\n",
    "    inputs = layers.Input(shape=(height, width, channels))\n",
    "    #xx = layers.Dense(3,activation='sigmoid')(inputs)\n",
    "\n",
    "    # >3 channels\n",
    "    #     input1 = layers.Conv2D(channels,(3,3),padding='same')(inputs)\n",
    "    #     input2 = layers.Conv2D(channels,(3,3),padding='same')(input1)\n",
    "    #     input = input = layers.Conv2D(3,(1,1),padding='same')(input2)\n",
    "    #     skips = base_model(input)\n",
    "\n",
    "    # 3 channels\n",
    "    skips = base_model(inputs)\n",
    "\n",
    "    x = skips[0]\n",
    "\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    for layer in upsample(512, (3, 3)):\n",
    "        x = layer(x)\n",
    "\n",
    "    for ch, skip in zip([256, 128, 64, 32], skips[1:-1]):\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "        for layer in upsample(ch, (3, 3)):\n",
    "            x = layer(x)\n",
    "\n",
    "    x = layers.Concatenate()([x, skips[-1]])\n",
    "    x = layers.Conv2D(96, (1, 1), padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2D(classes, (3, 3), padding='same', activation='softmax')(x)\n",
    "    #x = layers.Dense(classes, activation='sigmoid')(x)\n",
    "\n",
    "    unet = tf.keras.Model(inputs=inputs, outputs=x, name=name)\n",
    "    #unet.summary()\n",
    "    base_model.trainable = True\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9)\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    custom_cost_function\n",
    "\n",
    "    unet.compile(loss=custom_cost_function, optimizer=opt, metrics=['acc', kappa])\n",
    "    # unet.compile(loss=custom_cost_function, optimizer=opt, metrics=['acc', kappa, iou_0, iou_1])\n",
    "    print(unet.summary())\n",
    "    return unet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz, pydot\n",
    "\n",
    "# down_stack = def_basem()\n",
    "# model = def_unet(2,imgs,imgs,CH_N,down_stack,'unet16')\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE THE ANNOTATION IMAGES\n",
    "def combinemsk(whole_mskl):\n",
    "    co = 0\n",
    "    for i in range(45):\n",
    "        for r in range(48):\n",
    "            msk = tf.io.read_file(whole_mskl[co])\n",
    "            msk = tf.image.decode_jpeg(msk, channels=1)\n",
    "            msk = tf.image.convert_image_dtype(msk, tf.float32)\n",
    "            msk = tf.math.logical_and(msk < 256, msk > 0)\n",
    "            mskn = msk.numpy().squeeze()\n",
    "            if r == 0:\n",
    "                temp = mskn\n",
    "            else:\n",
    "                temp = np.concatenate((temp, mskn), axis=0)\n",
    "            co += 1\n",
    "        if i == 0:\n",
    "            compmsk = temp\n",
    "        else:\n",
    "            compmsk = np.concatenate((compmsk, temp), axis=1)\n",
    "\n",
    "    return compmsk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE THE AERIAL IMAGES\n",
    "def combinetif(whole_imgl):\n",
    "    dst = Image.new('RGBX', (256 * 45, 256 * 48))\n",
    "    co = 0\n",
    "    for i in range(45):\n",
    "        temp = Image.new('RGBX', (256, 256 * 48))\n",
    "        for r in range(48):\n",
    "            img2 = Image.open(whole_imgl[co])\n",
    "            co += 1\n",
    "            temp.paste(img2, (0, 256 * r))\n",
    "        dst.paste(temp, (i * 256, 0))\n",
    "    return dst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE THE SEGMENTATION RESULT IMAGES\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "\n",
    "def wholepic(w_ds, model, vstep, threshold=None):\n",
    "    allm = []\n",
    "    y_true = []\n",
    "    for _, m in w_ds.take(vstep):\n",
    "        # try:\n",
    "        #   y_true = np.concatenate((y_true,m.numpy()))\n",
    "        # except:\n",
    "        #   y_true = m.numpy()\n",
    "        for r in m:\n",
    "            allm.append(r)\n",
    "    wout = model.predict(w_ds, verbose=0, steps=vstep)\n",
    "    if threshold == None:\n",
    "        w_output = tf.argmax(wout, axis=3)\n",
    "    else:\n",
    "        w_output = wout[:, :, :, 1] > threshold\n",
    "\n",
    "    # CALCULATE THE SEGMENTATION RESULT MATRICS AFTER COMBINE\n",
    "    w_output = tf.cast(w_output, tf.int8)\n",
    "    # ytflat = y_true.flatten().astype('int8')\n",
    "    # woflat = w_output.numpy().flatten()\n",
    "    # print(pd.crosstab(ytflat, woflat, rownames = ['label'], colnames = ['predict'])) #CONFUSION MATRIX\n",
    "    w_output = tf.expand_dims(w_output, -1)\n",
    "    # los = keras.losses.sparse_categorical_crossentropy(allm,wout)\n",
    "    t1 = time.time()\n",
    "    kap = kappa(allm, w_output, False).numpy()\n",
    "    t2 = time.time()\n",
    "    print('kap:', kap, 'time:', (t2 - t1))\n",
    "\n",
    "    del wout, allm\n",
    "\n",
    "    co = 0\n",
    "    for i in range(45):\n",
    "        for r in range(48):\n",
    "            mskn = w_output[co]\n",
    "            mskn = tf.squeeze(mskn, -1)\n",
    "            co += 1\n",
    "            if r == 0:\n",
    "                temp = mskn\n",
    "            else:\n",
    "                temp = np.concatenate((temp, mskn), axis=0)\n",
    "        if i == 0:\n",
    "            compmsk2 = temp\n",
    "        else:\n",
    "            compmsk2 = np.concatenate((compmsk2, temp), axis=1)\n",
    "\n",
    "    # return compmsk2,[kap, accuracy_score(ytflat,woflat), iou0, iou1, pre,rec]\n",
    "    return compmsk2, [kap]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.enable()\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRC_kappa(tds, vstep, model):\n",
    "    y_trueo = []\n",
    "    for _, m in tds.take(vstep):\n",
    "        try:\n",
    "            y_trueo = np.concatenate((y_trueo, m.numpy()))\n",
    "        except:\n",
    "            y_trueo = m.numpy()\n",
    "    wout = model.predict(tds, verbose=0, steps=vstep)\n",
    "    y_true = y_trueo.ravel()\n",
    "    y_score = wout[:, :, :, 1].ravel()\n",
    "    del wout\n",
    "    del y_trueo\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    ### CALCULATE CONFUSION MATRICS / KAPPA FOR DIFFERENT THRESHOLDS/ MODIFIED FROM SKLEARN \n",
    "    y_true = (y_true == 1)\n",
    "    desc_score_indices = np.argsort(y_score)[::-1]\n",
    "    #y_score = y_score[desc_score_indices]\n",
    "    #y_true = y_true[desc_score_indices]\n",
    "    np.take(y_score, desc_score_indices, out=y_score)\n",
    "    np.take(y_true, desc_score_indices, out=y_true)\n",
    "    distinct_value_indices = np.where(np.diff(y_score))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n",
    "    tps = stable_cumsum(y_true)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps\n",
    "    del distinct_value_indices\n",
    "    del desc_score_indices\n",
    "\n",
    "    gc.collect()\n",
    "    y_true2 = y_true[::-1]\n",
    "    del y_true\n",
    "    tns = stable_cumsum(np.invert(y_true2))[::-1][threshold_idxs]\n",
    "    #fns = stable_cumsum(1-np.invert(y_true2))[::-1][threshold_idxs]\n",
    "    total = tps[-1] + tps[0] + fps[0] + tns[0]\n",
    "    fns = total - fps - tns - tps\n",
    "    del y_true2\n",
    "    del total\n",
    "\n",
    "    gc.collect()\n",
    "    total = (tns + tps + fns + fps)[0]\n",
    "    po = (tns + tps) / total\n",
    "    p1 = ((tps + fps) / total) * ((tps + fns) / total)\n",
    "    p2 = ((tns + fps) / total) * ((tns + fns) / total)\n",
    "    pe = p1 + p2\n",
    "    kap = (po - pe) / (1 - pe)\n",
    "\n",
    "    optthresh = y_score[threshold_idxs][np.argmax(kap)]\n",
    "    print('Best Kappa: %.3f  when threshold = %.3f ' % (max(kap), optthresh))\n",
    "    # print('Best Kappa: %.3f  when threshold = %.3f  precision: %.3f  recall: %.3f' % (max(kap),optthresh,precision[np.argmax(kap)],recall[np.argmax(kap)]))\n",
    "\n",
    "    return optthresh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT EXP RESULT\n",
    "def writefile(fold, res, rtype):\n",
    "    res1 = res[0]\n",
    "\n",
    "    global dst\n",
    "    workbook = openpyxl.load_workbook(dst)\n",
    "    global sheet_name\n",
    "    sheet = workbook[sheet_name]\n",
    "\n",
    "    if rtype == 'validation':\n",
    "        shift = 8\n",
    "    else:\n",
    "        shift = 4\n",
    "    for col in range(len(res1)):\n",
    "        sheet.cell(column=(col + shift), row=(fold + 3), value=\"%.4f\" % res1[col])\n",
    "\n",
    "    workbook.save(dst)\n",
    "\n",
    "\n",
    "def writefile_train(fold, trains, vals, hist):\n",
    "    global dst\n",
    "    workbook = openpyxl.load_workbook(dst)\n",
    "    global sheet_name\n",
    "    sheet = workbook[sheet_name]\n",
    "    string = 'train:'\n",
    "    for i in range(len(trains)):\n",
    "        if i == 0:\n",
    "            string = string + str(trains[i])\n",
    "        else:\n",
    "            string = string + ', ' + str(trains[i])\n",
    "    string = string + \"\\nval:{}\".format(vals[0])\n",
    "    sheet.cell(column=2, row=(fold + 3), value=string)\n",
    "\n",
    "    res = []\n",
    "    hist = hist.history\n",
    "    res.append(sum(hist['loss']) / len(hist['loss']))\n",
    "    res.append(sum(hist['kappa']) / len(hist['kappa']))\n",
    "    res.append(sum(hist['acc']) / len(hist['acc']))\n",
    "\n",
    "    shift = 13\n",
    "    for col in range(len(res)):\n",
    "        sheet.cell(column=(col + shift), row=(fold + 3), value=\"%.4f\" % res[col])\n",
    "    workbook.save(dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCH_N = 1  #20\n",
    "\n",
    "\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "def my_train(fold, dirp):\n",
    "    t1 = time.time()\n",
    "    for i in fold:\n",
    "        print(\"=== FOLD %d ===\" % (i + 1))\n",
    "        bm = def_basem()\n",
    "        model = def_unet(2, imgs, imgs, CH_N, bm, 'unet')\n",
    "\n",
    "        NOTE = ('FINAL_5FC4')\n",
    "\n",
    "        vals = [x for x in range(5) if x % 5 == i]\n",
    "\n",
    "        #         rest = [x for x in range(4) if x not in tests]\n",
    "        #         random.shuffle(rest)\n",
    "        #         vals = rest[:1]\n",
    "        #         trains = [x for x in rest if x not in vals]\n",
    "\n",
    "        print('Vals ', vals, '\\n Trains ', trains)\n",
    "        imgtr_ds, imgval_ds = create_dataset(BATCH_SIZE, imgs, CH_N, trains, vals)\n",
    "        imgtr_ds = imgtr_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        mc = tf.keras.callbacks.ModelCheckpoint((dirp + r\"/Models/U_BS%d_I%d_%s.h5\" % (BATCH_SIZE, imgs, NOTE)),\n",
    "                                                monitor='val_kappa', mode='max', verbose=1, save_best_only=True)\n",
    "        mc2 = tf.keras.callbacks.ModelCheckpoint((dirp + r\"/Models/U_BS%d_I%d_%s\" % (BATCH_SIZE, imgs, NOTE)),\n",
    "                                                 monitor='val_kappa', mode='max', verbose=1, save_best_only=True)\n",
    "        ulogdir = dirp + r'/logs/U_BS%d_I%d_%s' % (BATCH_SIZE, imgs, NOTE)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(ulogdir, histogram_freq=1)\n",
    "\n",
    "        VALIDATION_STEPS = math.ceil(len(w_imgl[0]) * 1 / BATCH_SIZE)\n",
    "        STEPS_PER_EPOCH = math.ceil(len(w_imgl[0]) * 5 / BATCH_SIZE)\n",
    "\n",
    "        historyt = model.fit(imgtr_ds, epochs=EPOCH_N,\n",
    "                             steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                             validation_steps=VALIDATION_STEPS,\n",
    "                             validation_data=imgval_ds,\n",
    "                             callbacks=[mc, mc2, tensorboard_callback]  #mc,\n",
    "                             )\n",
    "        history.append(historyt)\n",
    "\n",
    "        model.save(root + 'Models/U_BS%d_I%d_%s' % (BATCH_SIZE, imgs, NOTE))\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.load_weights(dirp + r'/Models/U_BS%d_I%d_%s.h5' % (BATCH_SIZE, imgs, NOTE))\n",
    "\n",
    "        writefile_train(fold[0], trains, vals, historyt)\n",
    "        #     for t in history:\n",
    "        #       print(t.history)\n",
    "\n",
    "        t2 = time.time()\n",
    "\n",
    "        # VALIDATION\n",
    "        gc.collect()\n",
    "        vresult = []\n",
    "        opvalTH = PRC_kappa(imgval_ds, VALIDATION_STEPS, model)\n",
    "        custom_TH.append(opvalTH)\n",
    "\n",
    "        for j in range(1):\n",
    "            print(\"Validation %d\" % (j + 1))\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.rc('font', size=14)\n",
    "            plt.imshow(combinetif(w_imgl[vals[j]]))# only to show\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # OUTPUT COLOR\n",
    "            aa = np.full([12288, 11520, 3], 255, 'uint8')\n",
    "            c1 = np.full([12288, 11520, 1], 213, 'uint8')\n",
    "            c2 = np.full([12288, 11520, 1], 180, 'uint8')\n",
    "            c3 = np.full([12288, 11520, 1], 60, 'uint8')\n",
    "            cc = np.concatenate((c1, c2, c3), axis=2)\n",
    "\n",
    "            # PREDICTION\n",
    "            tds = create_valds(BATCH_SIZE, imgs, CH_N, vals, j)\n",
    "            mask = combinemsk(w_mskl[vals[j]]) # only to show\n",
    "            # I add the batch size\n",
    "\n",
    "            valresult = model.evaluate(tds, steps=math.ceil(len(w_imgl[0]) / BATCH_SIZE))\n",
    "            out, met = wholepic(tds, model, math.ceil(len(w_imgl[0]) / BATCH_SIZE),\n",
    "                                opvalTH)  #met = Metrics of the output\n",
    "            out1 = (tf.image.resize(np.expand_dims(out, -1), (12288, 11520)) > 0.5).numpy().astype(\n",
    "                'uint8')  #11520x12288\n",
    "\n",
    "            # SHOW THE VALIDATION GT AND RESULT\n",
    "            plt.figure(figsize=(16, 8))\n",
    "            plt.rc('font', size=14)\n",
    "            a = plt.subplot(1, 2, 1)\n",
    "            a.set_title('True Mask')\n",
    "            a = plt.imshow(mask)\n",
    "            b = plt.subplot(1, 2, 2)\n",
    "            b.set_title('Predicted Mask')\n",
    "            b = plt.imshow(np.squeeze(out1))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            # SAVE RESULT the image\n",
    "            rout = np.where(out1, cc, aa)\n",
    "            im = Image.fromarray(rout)\n",
    "            im = im.resize((11460, 12260), Image.NEAREST)\n",
    "            im.save(dirp + r'/Val/TH_Fold%d_VAL%d_IMG%d.png' % ((i + 1), (j + 1), vals[j]))\n",
    "            # plt.imsave(root + r'/Fold%d_VAL%d_IMG%d_TH.png' % ((i+1), (j+1), vals[j]), rout)\n",
    "            del c1, c2, c3, out, out1, rout\n",
    "\n",
    "            # LOSS + KAP + ACC\n",
    "            vresult.append([valresult[0]] + met)\n",
    "            # vresult_p.append([valresult[0]] + met_p)\n",
    "            writefile(fold[0], vresult, 'validation')\n",
    "\n",
    "            gc.collect()\n",
    "        val_result.append(vresult)\n",
    "        #val_result_o.append(vresult_p)\n",
    "        print('Val Custom TH', val_result)\n",
    "        t3 = time.time()\n",
    "        print(\"time: \", (t2 - t1), \", \", (t3 - t2))\n",
    "        print('custom_TH:', custom_TH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "val_result = []\n",
    "teresult = []\n",
    "teresult_p = []\n",
    "custom_TH = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print(dirp)\n",
    "# print(dst, \": \", sheet_name)\n",
    "\n",
    "trains = [0, 1, 2, 3, 4]\n",
    "my_train([0], dirp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# trains = [2,3]\n",
    "# vals = [0]\n",
    "# my_train([1], dirp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# trains = [0,3]\n",
    "# vals = [1]\n",
    "# my_train([2], dirp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# trains = [0,1]\n",
    "# vals = [2]\n",
    "# my_train([3], dirp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Val Custom TH', val_result, '\\n Test Custom TH', teresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# for t in history:\n",
    "#   print(t.history)\n",
    "\n",
    "for i in history:\n",
    "    for r in i.history.values():\n",
    "        print('%.3f' % r[-1], end=' ')\n",
    "#   print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "valme = [val_result]\n",
    "teme = [teresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Validation\\n\")\n",
    "for i in range(1):\n",
    "    print('Fold %d' % (i + 1))\n",
    "    # print('Loss Acc Kappa IoU0 IoU1 Preci(0) Recall (0) Preci(1) Recall (1)')\n",
    "    print('Loss Kappa')\n",
    "    for l in range(1):\n",
    "        for t in valme:\n",
    "            print('%.3f %.3f' % (t[i][l][0], t[i][l][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# crilist3=['Loss','Accuracy','Kappa','IoU_0','IoU_1']\n",
    "crilist2 = ['loss', 'acc', 'kappa']\n",
    "crilist3 = ['Loss', 'Accuracy', 'Kappa']\n",
    "plt.rc('font', size=20)\n",
    "for n, r in enumerate(history):\n",
    "    plt.figure(figsize=(11, 10))\n",
    "    plt.suptitle('Fold %d Training' % ((n + 1)), fontsize=25)\n",
    "    for i in crilist2:\n",
    "        try:\n",
    "            plt.plot(np.array(r[i]), linewidth=3)\n",
    "        except:\n",
    "            plt.plot(np.array(r.history[i]), linewidth=3)\n",
    "    plt.xticks(range(20), range(1, 21))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(crilist3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# crilist3=['Loss','Accuracy','Kappa','IoU_0','IoU_1']\n",
    "crilist2 = ['loss', 'acc', 'kappa']\n",
    "crilist3 = ['Loss', 'Accuracy', 'Kappa']\n",
    "plt.rc('font', size=20)\n",
    "for n, r in enumerate(history):\n",
    "    plt.figure(figsize=(11, 10))\n",
    "    plt.suptitle('Fold %d Validation' % ((n + 1)), fontsize=25)\n",
    "    for i in crilist2:\n",
    "        try:\n",
    "            plt.plot(np.array(r['val_' + i]), linewidth=3)\n",
    "        except:\n",
    "            plt.plot(np.array(r.history['val_' + i]), linewidth=3)\n",
    "    plt.xticks(range(20), range(1, 21))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(crilist3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Sensitivity Analysis we use 5 images of Real Testing\n",
    "imgfiles = [f for f in glob.glob(r'/work/u8104687/24_IMG_CROP/*.png')]\n",
    "mskfiles = [f for f in glob.glob(r'/work/u8104687/24_MASK_CROP/*.png')]\n",
    "\n",
    "imgfiles.sort()\n",
    "mskfiles.sort()\n",
    "print(len(imgfiles), len(mskfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n_pic = len(imgfiles) // 2160\n",
    "whole_imgl = [{} for _ in range(n_pic)]\n",
    "whole_mskl = [{} for _ in range(n_pic)]\n",
    "for r, i in enumerate(zip(imgfiles, mskfiles)):\n",
    "    whole_imgl[r % n_pic][int(i[0].split('/')[-1].split('_')[0])] = (i[0])\n",
    "    whole_mskl[r % n_pic][int(i[0].split('/')[-1].split('_')[0])] = (i[1])\n",
    "w_imgl = [[x.get(i) for i in range(1, len(x) + 1, 1)] for x in whole_imgl]\n",
    "w_mskl = [[x.get(i) for i in range(1, len(x) + 1, 1)] for x in whole_mskl]\n",
    "print(len(w_imgl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def my_test(fold, opvalTH, tests, dirp):\n",
    "    teresult = []\n",
    "    t1 = time.time()\n",
    "    for i in fold:\n",
    "        print(\"=== FOLD %d ===\" % (i + 1))\n",
    "        bm = def_basem()\n",
    "        model = def_unet(2, imgs, imgs, CH_N, bm, 'unet')\n",
    "        NOTE = ('FINAL_5FC4')\n",
    "\n",
    "        #         model_path = glob.glob(dirp + r'/Models/U_BS64_I256_model{}.h5'.format(int(fold[0])+1))\n",
    "        model_path = glob.glob(dirp + r'/Models/U_BS64_I256_FINAL_5FC4.h5')\n",
    "\n",
    "        print(\"Model: \", model_path[0])\n",
    "        model.load_weights(model_path[0])\n",
    "\n",
    "        aa = np.full([12288, 11520, 3], 255, 'uint8')\n",
    "        c1 = np.full([12288, 11520, 1], 213, 'uint8')\n",
    "        c2 = np.full([12288, 11520, 1], 180, 'uint8')\n",
    "        c3 = np.full([12288, 11520, 1], 60, 'uint8')\n",
    "        cc = np.concatenate((c1, c2, c3), axis=2)\n",
    "        t3 = time.time()\n",
    "\n",
    "        # TESTING\n",
    "        tresult = []\n",
    "        pmet = []\n",
    "        omet = []\n",
    "        for j in range(len(tests)):\n",
    "            print(j)\n",
    "            print(\"Test %d\" % (j + 1))\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.rc('font', size=14)\n",
    "            plt.imshow(combinetif(w_imgl[tests[j]]))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            tds = create_testds(BATCH_SIZE, imgs, CH_N, tests, j)\n",
    "            #       result = model.evaluate(tds, steps = math.ceil(len(w_imgl[0])/BATCH_SIZE))\n",
    "            out, met = wholepic(tds, model, math.ceil(len(w_imgl[0]) / BATCH_SIZE), opvalTH)\n",
    "\n",
    "            # SHOW THE TESTING GT AND RESULT\n",
    "            plt.figure(figsize=(18, 18))\n",
    "            plt.rc('font', size=14)\n",
    "            a = plt.subplot(1, 2, 1)\n",
    "            a.set_title('True Mask')\n",
    "            allmask = combinemsk(w_mskl[tests[j]])\n",
    "            a = plt.imshow(allmask)\n",
    "            b = plt.subplot(1, 2, 2)\n",
    "            b.set_title('Predicted Original')\n",
    "\n",
    "            out1 = (tf.image.resize(np.expand_dims(out, -1), (12288, 11520)) > 0.5).numpy().astype(\n",
    "                'uint8')  #11460 x #12260\n",
    "            b = plt.imshow(np.squeeze(out))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            rout = np.where(out1, cc, aa)\n",
    "            plt.imsave(dirp + '/Test/TH_Fold%d_REAL TEST JIAYI%d.png' % ((i + 1), (j + 1)), rout)\n",
    "            del out1, rout\n",
    "\n",
    "            # ------------------------------------------------------------------------------------------------------------------------\n",
    "            # KERNEL 10\n",
    "            close10 = cv2.morphologyEx(np.uint8(out), cv2.MORPH_OPEN,\n",
    "                                       cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10)))  #Remove\n",
    "            out_p10 = cv2.morphologyEx(np.uint8(close10), cv2.MORPH_CLOSE,\n",
    "                                       cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (10, 10)))  #Close\n",
    "            del out, close10\n",
    "\n",
    "            plt.figure(figsize=(18, 18))\n",
    "            plt.rc('font', size=14)\n",
    "            a = plt.subplot(1, 2, 1)\n",
    "            a.set_title('True Mask')\n",
    "            # allmask = combinemsk(w_mskl[tests[j]])\n",
    "            a = plt.imshow(allmask)\n",
    "            b = plt.subplot(1, 2, 2)\n",
    "            b.set_title('Predicted Post Processing 10')\n",
    "            b = plt.imshow(np.squeeze(out_p10))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            pmet_10 = kappa(allmask, out_p10, False).numpy()\n",
    "            # omet = kappa(allmask, out, False).numpy()\n",
    "            print('KAPPA Post Processing 10 = %.3f' % pmet_10)\n",
    "            print('KAPPA Original = %.3f' % met[0])\n",
    "            #       met.insert(1, pmet_10)\n",
    "\n",
    "            out10 = (tf.image.resize(np.expand_dims(out_p10, -1), (12288, 11520)) > 0.5).numpy().astype(\n",
    "                'uint8')  #11460 x #12260\n",
    "            rout10 = np.where(out10, cc, aa)\n",
    "            plt.imsave(dirp + '/Test/TH_Fold%d_REAL TEST JIAYI%d_IMG%d_Post10.png' % ((i + 1), (j + 1), tests[j]),\n",
    "                       rout10)\n",
    "            plt.clf()\n",
    "\n",
    "            tresult.append(pmet_10)\n",
    "\n",
    "        #         tresult.append((tresult[0]+tresult[1])/2)\n",
    "        tresult.append((tresult[0] + tresult[1] + tresult[2] + tresult[3]) / 4)\n",
    "        #         writefile(fold[0], [tresult], 'testing')\n",
    "\n",
    "        gc.collect()\n",
    "        teresult.append(tresult)\n",
    "        print('Test Custom TH', teresult)\n",
    "        t4 = time.time()\n",
    "        print(\"time: \", (t4 - t3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Real Testing for MODEL 1\n",
    "optthresh = custom_TH[0]\n",
    "my_test([0], optthresh, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], dirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# #Real Testing for FOLD 2\n",
    "# optthresh=custom_TH[1]\n",
    "# my_test([1], optthresh, [0,1,2,3,4], dirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# #Real Testing for FOLD 3\n",
    "# optthresh=custom_TH[2]\n",
    "# my_test([2], optthresh, [0,1,2,3,4], dirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# #Real Testing for FOLD 4\n",
    "# optthresh=custom_TH[3]\n",
    "# my_test([3], optthresh, [0,1,2,3,4], dirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# #COA REAL TESTING 59\n",
    "\n",
    "# imgfiles = [f for f in glob.glob(r'/work/u8104687/IMG_COA_REAL_TEST/*.png')]\n",
    "# mskfiles = [f for f in glob.glob(r'/work/u8104687/MASK_COA_REAL_TEST/*.png')]\n",
    "# imgfiles.sort()\n",
    "# mskfiles.sort()\n",
    "# print(len(imgfiles),len(mskfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# n_pic = len(imgfiles)//2160\n",
    "# whole_imgl = [ {} for _ in range(n_pic)]\n",
    "# whole_mskl = [ {} for _ in range(n_pic)]\n",
    "# for r,i in enumerate(zip(imgfiles,mskfiles)):\n",
    "#     whole_imgl[r%n_pic][int(i[0].split('/')[-1].split('_')[0])]=(i[0])\n",
    "#     whole_mskl[r%n_pic][int(i[0].split('/')[-1].split('_')[0])]=(i[1])\n",
    "# w_imgl = [[x.get(i) for i in range(1,len(x)+1,1)] for x in whole_imgl]\n",
    "# w_mskl = [[x.get(i) for i in range(1,len(x)+1,1)] for x in whole_mskl]\n",
    "# print(len(w_imgl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# optthresh=custom_TH[3]\n",
    "# my_test([3], optthresh, [0], dirp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}